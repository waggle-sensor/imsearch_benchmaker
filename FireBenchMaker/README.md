# FireBench Maker Tool

A comprehensive pipeline for creating image retrieval benchmarks in the fire science domain. FireBench transforms raw wildfire images into a structured benchmark dataset with natural language queries, relevance labels, and rich metadata.

## Table of Contents

- [Overview](#overview)
- [Pipeline Architecture](#pipeline-architecture)
- [Quick Start](#quick-start)
- [Detailed Pipeline Walkthrough](#detailed-pipeline-walkthrough)
- [Configuration](#configuration)
- [File Formats](#file-formats)
- [Make Commands Reference](#make-commands-reference)
- [Troubleshooting](#troubleshooting)
- [Cost Estimation](#cost-estimation)
- [References](#references)

## Overview

### What is FireBench?

FireBench is a benchmark dataset for evaluating **text-to-image retrieval systems** in fire science. Given a natural language query like "Fixed long-range daytime webcam images of mountainous shrubland with no visible smoke or flames", the system should retrieve relevant images from a candidate pool.

Final Dataset: [FireBench](https://huggingface.co/datasets/sagecontinuum/FireBench)

### What Does This Tool Do?

This tool automates the creation of FireBench datasets by:

1. **Annotating images** with rich metadata (summaries, tags, environmental conditions)
2. **Generating realistic queries** written from a fire scientist's perspective
3. **Labeling relevance** for query-image pairs (relevant vs. not relevant)
4. **Computing similarity scores** using CLIP models
5. **Preparing datasets** for Hugging Face Hub upload

### Key Features

- **Automated annotation** using OpenAI's vision API
- **Expert-style queries** generated by AI models trained on fire science context
- **Comprehensive metadata** including environmental conditions, smoke characteristics, confounders
- **False positive challenges** with hard negatives (fog, haze, clouds that look like smoke)
- **Batch processing** with automatic sharding for large datasets
- **Retry mechanisms** for handling API failures
- **EDA tools** for dataset analysis

## Pipeline Architecture

### High-Level Flow

```
Raw Images
    ↓
[1] Preprocess → images.jsonl, seeds.jsonl
    ↓
[2] Vision Annotation → annotations.jsonl (per-image metadata)
    ↓
[3] Query Planning → query_plan.jsonl (candidate pools per query)
    ↓
[4] Judge Generation → firebench_qrels.jsonl (queries + relevance labels)
    ↓
[5] Postprocessing → firebench_qrels_with_clipscore.jsonl (+ CLIP scores)
    ↓
[6] EDA → summary/ (visualizations and statistics)
    ↓
[7] Hugging Face Prep → hf_dataset/ (ready for upload)
```

### Stage-by-Stage Breakdown

#### Stage 1: Preprocess
**Input**: Folder of raw images  
**Output**: `images.jsonl`, `seeds.jsonl`  
**Purpose**: Create structured image inventory and select seed images for queries

#### Stage 2: Vision Annotation
**Input**: `images.jsonl`  
**Output**: `annotations.jsonl`  
**Purpose**: Extract rich metadata from each image (summaries, tags, environmental facets)

#### Stage 3: Query Planning
**Input**: `annotations.jsonl`, `seeds.jsonl`  
**Output**: `query_plan.jsonl`  
**Purpose**: Build candidate pools for each query (seed + hard/easy/nearmiss negatives)

#### Stage 4: Judge Generation
**Input**: `query_plan.jsonl`, `annotations.jsonl`  
**Output**: `firebench_qrels.jsonl`  
**Purpose**: Generate queries and label relevance for each query-image pair

#### Stage 5: Postprocessing
**Input**: `firebench_qrels.jsonl`  
**Output**: `firebench_qrels_with_clipscore.jsonl`, `summary/`  
**Purpose**: Add CLIP similarity scores and generate dataset analysis

#### Stage 6: Hugging Face Preparation
**Input**: `firebench_qrels_with_clipscore.jsonl`, `images.jsonl`  
**Output**: `hf_dataset/`  
**Purpose**: Prepare dataset for Hugging Face Hub upload

## Quick Start

### Prerequisites

1. **Python 3.11+** with required packages (see `requirements.txt`)
2. **OpenAI API key** (set in `.keys` file or environment variable)
3. **Image data** (download from sources listed in [References](#references))

### Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Set up environment variables
source .keys
```

### Run Full Pipeline

```bash
# Set required variables (or edit Makefile defaults)
export IMAGE_ROOT_DIR=/path/to/your/images
export IMAGES_JSONL=/path/to/outputs/images.jsonl
export SEEDS_JSONL=/path/to/outputs/seeds.jsonl

# Run everything
make all
```

This will:
1. Preprocess images → create `images.jsonl` and `seeds.jsonl`
2. Annotate images → create `annotations.jsonl`
3. Plan queries → create `query_plan.jsonl`
4. Generate queries and labels → create `firebench_qrels.jsonl`
5. Add CLIP scores → create `firebench_qrels_with_clipscore.jsonl`
6. Generate EDA → create `summary/` directory
7. Prepare for Hugging Face → create `hf_dataset/` directory

## Detailed Pipeline Walkthrough

### Stage 1: Preprocess

**Command**: `make preprocess`

**What it does**:
- Scans `IMAGE_ROOT_DIR` for image files
- Creates `images.jsonl` with image metadata (ID, URL, license, DOI)
- Selects `NUM_SEEDS` (default: 100) seed images uniformly across the dataset
- Creates `seeds.jsonl` mapping query IDs to seed image IDs

**Input**:
- Directory of images (e.g., `/path/to/images/`)
- Optional: `META_JSON` file mapping image paths to licenses/DOIs

**Output**:
- `images.jsonl`: One line per image
  ```json
  {"image_id": "figlib/image1.jpg", "image_url": "https://...", "mime_type": "image/jpeg", "license": "CC BY 4.0", "doi": "2112.08598"}
  ```
- `seeds.jsonl`: One line per query
  ```json
  {"query_id": "firebench_q001", "seed_image_ids": ["figlib/image1.jpg"]}
  ```

**Configuration**:
- `FIREBENCH_NUM_SEEDS`: Number of seed images (default: 100)
- `FIREBENCH_IMAGE_BASE_URL`: Base URL for public image access
- `IMAGE_ROOT_DIR`: Root directory containing images

**Notes**:
- Image IDs are POSIX relative paths from the root directory
- Seed selection is deterministic (sorted by image_id, then uniformly sampled)
- Images with `%` in their ID are skipped (to avoid URL encoding issues)

---

### Stage 2: Vision Annotation

**Command**: `make vision` (or step-by-step: `make vision-make vision-submit vision-wait vision-download vision-parse`)

**What it does**:
- Sends each image to OpenAI's vision API
- Extracts structured metadata: summaries, tags, environmental conditions
- Handles batch sharding automatically (to avoid 5M token limit)
- Supports retry for failed requests

**Input**: `images.jsonl`

**Output**: `annotations.jsonl` - One line per image:
```json
{
  "image_id": "figlib/image1.jpg",
  "summary": "Clear daytime view of mountainous shrubland...",
  "viewpoint": "fixed_long_range",
  "plume_stage": "none",
  "flame_visible": false,
  "lighting": "day",
  "confounder_type": "none",
  "environment_type": "mountainous",
  "tags": ["no_smoke_visible", "daylight", "clear_air", ...],
  "confidence": {
    "viewpoint": 0.9,
    "plume_stage": 0.95,
    "confounder_type": 0.9,
    "environment_type": 0.9
  },
  "license": "CC BY 4.0",
  "doi": "2112.08598"
}
```

**Key Fields**:
- `summary`: Factual description (≤30 words)
- `tags`: 12-18 tags from controlled vocabulary (e.g., "smoke_present", "mountainous", "daylight")
- `viewpoint`: Camera perspective (fixed_long_range, handheld, aerial, etc.)
- `plume_stage`: Smoke development stage (incipient, developing, mature, none, etc.)
- `environment_type`: Terrain type (forest, shrubland, mountainous, etc.)
- `confounder_type`: Potential false positive source (cloud, fog, haze, none, etc.)
- `confidence`: Per-field confidence scores (0-1)

**Batch Sharding**:
- Large batches are automatically split into shards (~900 images per shard)
- Shards are submitted with controlled concurrency
- All shards are downloaded and merged automatically

**Retry Mechanism**:
```bash
# If some images fail, retry only the failed ones
make vision-retry
```

**Configuration**:
- `OPENAI_VISION_MODEL`: Model to use (default: "gpt-5-mini")
- `FIREBENCH_MAX_IMAGES_PER_BATCH`: Max images per shard (default: 900)
- `FIREBENCH_MAX_CONCURRENT_BATCHES`: Concurrent shards (default: 1)
- `VISION_ANNOTATION_MAX_OUTPUT_TOKENS`: Max output tokens (default: 4000)

---

### Stage 3: Query Planning

**Command**: `make plan`

**What it does**:
- For each seed image, builds a candidate pool of images
- Includes the seed image + negative examples (hard, nearmiss, easy)
- Hard negatives: Images with confounders (fog, haze, clouds) that might be confused with smoke
- Nearmiss negatives: Similar images but not relevant
- Easy negatives: Clearly different images

**Input**: `annotations.jsonl`, `seeds.jsonl`

**Output**: `query_plan.jsonl` - One line per query:
```json
{
  "query_id": "firebench_q001",
  "seed_image_ids": ["figlib/image1.jpg"],
  "candidate_image_ids": [
    "figlib/image1.jpg",  // seed
    "figlib/image2.jpg",  // hard negative (fog looks like smoke)
    "figlib/image3.jpg",  // nearmiss negative (similar but different)
    "figlib/image4.jpg",  // easy negative (clearly different)
    // ... 40 total candidates
  ]
}
```

**Configuration**:
- `FIREBENCH_NEGATIVES_PER_QUERY`: Total negatives (default: 40)
- `FIREBENCH_HARD_NEG`: Hard negatives (default: 25)
- `FIREBENCH_NEARMISS_NEG`: Nearmiss negatives (default: 10)
- `FIREBENCH_EASY_NEG`: Easy negatives (default: 5)

**Notes**:
- A seed image is never selected as a negative for its own query
- But it can appear as a negative for other queries
- Selection uses metadata similarity (tags, environment, etc.)

---

### Stage 4: Judge Generation

**Command**: `make judge` (or step-by-step: `make judge-make judge-submit judge-wait judge-download judge-parse`)

**What it does**:
- For each query plan, sends seed image metadata to OpenAI's text API
- Generates a natural language query describing the target phenomenon
- Judges relevance for each candidate image (0 = not relevant, 1 = relevant)
- Creates the final benchmark dataset

**Input**: `query_plan.jsonl`, `annotations.jsonl`

**Output**: `firebench_qrels.jsonl` - One line per query-image pair:
```json
{
  "query_id": "firebench_q001",
  "query_text": "Fixed long-range daytime webcam images of mountainous shrubland with no visible smoke or flames",
  "image_id": "figlib/image1.jpg",
  "relevance_label": 1,
  "license": "CC BY 4.0",
  "doi": "2112.08598",
  "tags": ["no_smoke_visible", "daylight", ...],
  "confidence": {"viewpoint": 0.9, ...},
  "environment_type": "mountainous",
  "confounder_type": "none",
  "lighting": "day",
  "flame_visible": false,
  "plume_stage": "none",
  "viewpoint": "fixed_long_range",
  "summary": "Clear daytime view of mountainous shrubland..."
}
```

**Key Fields**:
- `query_text`: Natural language query (what a fire scientist would search for)
- `relevance_label`: Binary label (1 = relevant, 0 = not relevant)
- All metadata from `annotations.jsonl` is joined in

**Retry Mechanism**:
```bash
# If some queries fail, retry only the failed ones
make judge-retry
```

**Configuration**:
- `OPENAI_TEXT_MODEL`: Model to use (default: "gpt-5-mini")
- `FIREBENCH_MAX_QUERIES_PER_BATCH`: Max queries per shard (default: 600)
- `JUDGE_MAX_OUTPUT_TOKENS`: Max output tokens (default: 8000)

---

### Stage 5: Postprocessing

#### 5a. CLIPScore Calculation

**Command**: `make clipscore`

**What it does**:
- Computes CLIP similarity scores for each query-image pair
- Uses a local CLIP model (or HTTP API) to compute semantic similarity
- Adds `clip_score` column to the dataset

**Input**: `firebench_qrels.jsonl`, `images.jsonl`

**Output**: `firebench_qrels_with_clipscore.jsonl` - Same as above, plus:
```json
{
  ...
  "clip_score": 5.337447166442871
}
```

**Configuration**:
- `CLIP_ADAPTER`: Adapter to use ("local" or "http")
- `CLIP_ADAPTER_ARGS`: JSON string with adapter arguments
  ```json
  {"model": "apple/DFN5B-CLIP-ViT-H-14-378", "device": "cpu", "use_safetensors": true}
  ```

**Notes**:
- CLIP scores are raw dot product similarities (can be negative or positive)
- Higher scores indicate higher semantic similarity
- Scores are computed using the query text and image URL

#### 5b. Dataset Summary (EDA)

**Command**: `make summary`

**What it does**:
- Generates comprehensive exploratory data analysis
- Creates visualizations: distributions, word clouds, relevance analysis
- Includes CLIP score analysis and confidence analysis
- Exports statistics tables as CSV

**Input**: `firebench_qrels_with_clipscore.jsonl`, `images.jsonl`

**Output**: `summary/` directory with:
- `image_proportion_donuts.png`: Categorical distributions
- `query_relevancy_distribution.png`: Relevance per query
- `wordcloud_summaries.png`, `wordcloud_tags.png`: Word clouds
- `clipscore_analysis.png`: CLIP score distributions and analysis
- `confidence_by_category_boxplot.png`: Confidence score analysis
- `*.csv`: Statistics tables

---

### Stage 6: Hugging Face Preparation

**Command**: `make huggingface`

**What it does**:
- Prepares dataset for Hugging Face Hub upload
- Uses image URLs directly (no local downloading needed)
- Creates Hugging Face Dataset with Image feature type
- Optionally uploads to Hub if `HF_REPO_ID` is configured

**Input**: `firebench_qrels_with_clipscore.jsonl`, `images.jsonl`

**Output**: `hf_dataset/` directory:
- `dataset/`: Hugging Face dataset (ready to push)
- `dataset_metadata.json`: Dataset statistics

**Configuration**:
- `HF_REPO_ID`: Repository ID (e.g., "username/dataset-name") - set in `config.py` or environment
- `HF_TOKEN`: Hugging Face token - set in `.keys` file
- `HF_PRIVATE`: Whether repository is private (default: false) - set in `config.py` or environment

**Notes**:
- Images are referenced by URL (Hugging Face downloads them during upload)
- If `IMAGE_ROOT_DIR` is provided, local file paths are used instead
- Dataset includes all columns from qrels plus the image feature

## Configuration

### Environment Variables

Create a `.keys` file in the `FireBench/` directory:

```bash
# Required
OPENAI_API_KEY=sk-...

# Optional
HF_TOKEN=hf_...
```

### Config.py

All configuration is in `config.py` and can be overridden via environment variables. Key settings:

- **Models**: `VISION_MODEL`, `TEXT_MODEL` (default: "gpt-5-mini")
- **Batch sizes**: `MAX_IMAGES_PER_BATCH` (900), `MAX_QUERIES_PER_BATCH` (600)
- **Query planning**: `NEG_TOTAL` (40), `NEG_HARD` (25), `NEG_NEARMISS` (10), `NEG_EASY` (5)
- **CLIP**: `CLIP_ADAPTER`, `CLIP_ADAPTER_ARGS`
- **Hugging Face**: `HF_REPO_ID`, `HF_PRIVATE`

See `config.py` for full list of configurable parameters.

### Makefile Variables

Key variables you can override:

```bash
# Inputs
IMAGE_ROOT_DIR=/path/to/images
META_JSON=/path/to/rights_map.json  # Optional

# Outputs
IMAGES_JSONL=/path/to/outputs/images.jsonl
SEEDS_JSONL=/path/to/outputs/seeds.jsonl
ANNOTATIONS_JSONL=/path/to/outputs/annotations.jsonl
QRELS_JSONL=/path/to/outputs/firebench_qrels.jsonl
QRELS_WITH_CLIPSCORE_JSONL=/path/to/outputs/firebench_qrels_with_clipscore.jsonl
HF_DATASET_DIR=/path/to/outputs/hf_dataset
```

## File Formats

### images.jsonl
```json
{"image_id": "path/to/image.jpg", "image_url": "https://...", "mime_type": "image/jpeg", "license": "CC BY 4.0", "doi": "2112.08598"}
```

### seeds.jsonl
```json
{"query_id": "firebench_q001", "seed_image_ids": ["path/to/image.jpg"]}
```

### annotations.jsonl
```json
{
  "image_id": "path/to/image.jpg",
  "summary": "Brief description...",
  "viewpoint": "fixed_long_range",
  "plume_stage": "none",
  "flame_visible": false,
  "lighting": "day",
  "confounder_type": "none",
  "environment_type": "mountainous",
  "tags": ["tag1", "tag2", ...],
  "confidence": {"viewpoint": 0.9, ...},
  "license": "CC BY 4.0",
  "doi": "2112.08598"
}
```

### query_plan.jsonl
```json
{"query_id": "firebench_q001", "seed_image_ids": ["img1.jpg"], "candidate_image_ids": ["img1.jpg", "img2.jpg", ...]}
```

### firebench_qrels.jsonl
```json
{
  "query_id": "firebench_q001",
  "query_text": "Natural language query...",
  "image_id": "path/to/image.jpg",
  "relevance_label": 1,
  // ... all metadata from annotations.jsonl
}
```

### firebench_qrels_with_clipscore.jsonl
Same as above, plus:
```json
{"clip_score": 5.337447166442871}
```

## Make Commands Reference

### Full Pipeline
```bash
make all  # Runs: preprocess → vision → plan → judge → postprocess → huggingface
```

### Individual Stages
```bash
make preprocess    # Create images.jsonl and seeds.jsonl
make vision        # Annotate images (make → submit → wait → download → parse)
make plan          # Build query plans
make judge         # Generate queries and labels (make → submit → wait → download → parse)
make postprocess   # CLIPScore + EDA (clipscore → summary)
make huggingface   # Prepare for Hugging Face Hub
```

### Retry Commands
```bash
make vision-retry  # Retry failed vision annotations
make judge-retry   # Retry failed judge queries
```

### Helper Commands
```bash
make help          # Show all available commands
make env-check      # Validate OPENAI_API_KEY is set
make list-active    # List active OpenAI batches
make list-all       # List all OpenAI batches
make clean          # Remove intermediate outputs
```

### Step-by-Step (for debugging)
```bash
# Vision phase
make vision-make      # Create batch file
make vision-submit    # Submit to OpenAI
make vision-wait      # Wait for completion
make vision-download  # Download results
make vision-parse     # Parse to annotations.jsonl

# Judge phase
make judge-make       # Create batch file
make judge-submit     # Submit to OpenAI
make judge-wait       # Wait for completion
make judge-download   # Download results
make judge-parse      # Parse to firebench_qrels.jsonl
```

## Troubleshooting

### Common Issues

#### 1. Batch file too large
**Error**: "Batch file exceeds 200MB"  
**Solution**: The pipeline automatically shards large batches. If you see this error, check `MAX_IMAGES_PER_BATCH` in `config.py` (should be ≤900).

#### 2. Enqueued token limit
**Error**: "Batch rejected: enqueued token cap exceeded"  
**Solution**: This is handled automatically via batch sharding. If you see this, the shard size might be too large - reduce `MAX_IMAGES_PER_BATCH`.

#### 3. SSL connection errors
**Error**: `SSLV3_ALERT_BAD_RECORD_MAC`  
**Solution**: Transient network issue. The retry mechanism will handle this. Run `make vision-retry` or `make judge-retry` if needed.

#### 4. Missing images
**Error**: "Image not found" during Hugging Face prep  
**Solution**: Ensure `IMAGE_ROOT_DIR` points to the correct location, or that `images.jsonl` has valid URLs.

#### 5. CLIP scores all 1.0
**Issue**: All CLIP scores are the same  
**Solution**: Check that the CLIP adapter is correctly configured. The score should be a raw dot product (can be negative or positive), not normalized to [0,1].

### Debugging Tips

1. **Check batch status**: `make list-active` to see running batches
2. **Inspect intermediate files**: Check `*_err.jsonl` files for failed requests
3. **Validate inputs**: Use `make env-check` to verify configuration
4. **Check logs**: All commands use Python logging - look for WARNING/ERROR messages
5. **Test with small subset**: Reduce `NUM_SEEDS` to test the pipeline quickly

## Cost Estimation

### Reference Costs (3849 images, gpt-5-mini, 12/27/2025)

**Model pricing** (per 1M tokens):
- Input: $0.125
- Cached Input: $0.0125
- Output: $1.00

**Stage costs**:
- Vision annotation: $3.76
- Judge generation: $0.49
- **Total**: $4.25

**Cost factors**:
- Number of images (vision stage scales linearly)
- Number of queries (judge stage scales with queries × candidates)
- Model choice (gpt-5-mini is cost-effective)
- Image detail level (low detail saves tokens)
- Reasoning effort (low effort saves tokens)
- Max tokens (max tokens per image and query)

## Advanced Usage

### Using SSH/Remote Images

You can mount remote image directories using `sshfs`:

```bash
sshfs user@host:/path/to/images /tmp/FireBench/images
export IMAGE_ROOT_DIR=/tmp/FireBench/images
make preprocess
```

This allows running the pipeline locally without downloading all images.

### Custom CLIP Adapters

The CLIPScore calculation supports pluggable adapters. See `clipscore/` directory for:
- `LocalCLIPAdapter`: Uses local Hugging Face models
- `HTTPCLIPAdapter`: Uses HTTP API endpoints

To add a new adapter, implement the `CLIPAdapter` interface in `clipscore/framework/base.py`.

### Custom Prompts

You can customize the prompts used for vision annotation and judge generation by modifying:
- `VISION_ANNOTATION_SYSTEM_PROMPT` in `config.py`
- `VISION_ANNOTATION_USER_PROMPT` in `config.py`
- `JUDGE_SYSTEM_PROMPT` in `config.py`
- `JUDGE_USER_PROMPT` in `config.py`

## References

### Source Datasets

1. **HPWREN FIgLib**: https://www.hpwren.ucsd.edu/FIgLib/index.html
   - High-Performance Wireless Research and Education Network
   - Fixed long-range webcam imagery
   - DOI: 10.48550/arXiv.2112.08598

2. **The Wildfire Dataset**: https://www.kaggle.com/datasets/elmadafri/the-wildfire-dataset
   - Diverse wildfire imagery
   - DOI: 10.3390/f14091697

3. **Sage Continuum**: https://sagecontinuum.org
   - Array of Things sensor network
   - DOI: 10.1109/ICSENS.2016.7808975

### Citations

```
El-Madafri I, Peña M, Olmedo-Torre N. The Wildfire Dataset: Enhancing Deep Learning-Based Forest Fire Detection with a Diverse Evolving Open-Source Dataset Focused on Data Representativeness and a Novel Multi-Task Learning Approach. Forests. 2023; 14(9):1697. https://doi.org/10.3390/f14091697

Anshuman Dewangan, Yash Pande, Hans-Werner Braun, Frank Vernon, Ismael Perez, Ilkay Altintas, Garrison W Cottrell, and Mai H Nguyen. Figlib & smokeynet: Dataset and deep learning model for real-time wildland fire smoke detection. Remote Sensing, 14(4):1007, 2022. https://doi.org/10.48550/arXiv.2112.08598

Catlett, C. E., P. H. Beckman, R. Sankaran, and K. K. Galvin, 2017: Array of Things: A Scientific Research Instrument in the Public Way: Platform Design and Early Lessons Learned. Proceedings of the 2nd International Workshop on Science of Smart City Operations and Platforms Engineering, 26–33. https://doi.org/10.1109/ICSENS.2016.7808975
```

## Project Structure

```
FireBench/
├── preprocess.py          # Stage 1: Create images.jsonl and seeds.jsonl
├── firebench.py            # Stages 2 & 4: Vision and judge batch processing
├── build_query_plan.py     # Stage 3: Build candidate pools
├── postprocess.py          # Stages 5 & 6: CLIPScore, EDA, Hugging Face prep
├── config.py              # Configuration and constants
├── Makefile               # Pipeline automation
├── requirements.txt       # Python dependencies
├── .keys                  # Keys file (create from .keys.example)
└── tools/                 # Data download scripts
    ├── get_figlib.py
    └── get_sage.py

clipscore/                 # CLIPScore calculation framework
├── __init__.py
├── framework/
│   └── base.py            # CLIPAdapter interface
└── adapters/
    └── clip.py           # Local and HTTP adapters
```
